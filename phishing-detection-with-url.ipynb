{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9106778,"sourceType":"datasetVersion","datasetId":5496177},{"sourceId":9106973,"sourceType":"datasetVersion","datasetId":5496271}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\nimport pickle\nimport os\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport keras_tuner as kt\nimport matplotlib.pyplot as plt\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset as HFDataset\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\n# Download NLTK data files (if not already installed)\nnltk.download('stopwords')\nnltk.download('punkt')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:06:12.706677Z","iopub.execute_input":"2024-10-14T11:06:12.707092Z","iopub.status.idle":"2024-10-14T11:06:33.258482Z","shell.execute_reply.started":"2024-10-14T11:06:12.707048Z","shell.execute_reply":"2024-10-14T11:06:33.257574Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text, stop_words):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n    text = text.lower()\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return ' '.join(filtered_text)\n\ndef stem_text(text):\n    stemmer = PorterStemmer()\n    word_tokens = word_tokenize(text)\n    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n    return ' '.join(stemmed_text)\n\ndef ml_preprocess(df):\n    stop_words = set(stopwords.words('english'))\n    df['subject'].fillna('', inplace=True)\n    df['content'] = df['subject'] + ' ' + df['body']\n    df['content'] = df['content'].apply(lambda x: clean_text(x, stop_words))\n    df['content'] = df['content'].apply(stem_text)\n    df = df[['label', 'content']]\n    return df\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/phishing-email-dataset-nazario-5-and-trec07/Nazario_5.csv\")\n\n# Preprocess the dataset\ndf = ml_preprocess(df)\n\n# Split the data into training and testing sets\nX = df['content']\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:06:33.260294Z","iopub.execute_input":"2024-10-14T11:06:33.261076Z","iopub.status.idle":"2024-10-14T11:07:28.156014Z","shell.execute_reply.started":"2024-10-14T11:06:33.261031Z","shell.execute_reply":"2024-10-14T11:07:28.155211Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/571591972.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['subject'].fillna('', inplace=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Traditional ML Models","metadata":{}},{"cell_type":"code","source":"# Define the models\nrf_model = RandomForestClassifier(random_state=42)\nsvm_model = SVC(probability=True, random_state=42)\nknn_model = KNeighborsClassifier()\n\n# Create pipelines for each model\nrf_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', rf_model)\n])\n\nsvm_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', svm_model)\n])\n\nknn_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', knn_model)\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:29:02.587312Z","iopub.execute_input":"2024-10-14T10:29:02.587722Z","iopub.status.idle":"2024-10-14T10:29:02.593561Z","shell.execute_reply.started":"2024-10-14T10:29:02.587679Z","shell.execute_reply":"2024-10-14T10:29:02.592717Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Random Forrest","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for Grid Search\nrf_params = {\n    'classifier__n_estimators': [150,200, 300],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10]\n}\n\n# Perform Grid Search CV for Random Forest\nrf_grid = GridSearchCV(rf_pipeline, rf_params, cv=5, n_jobs=-1, verbose=1)\nrf_grid.fit(X_train, y_train)\nprint(\"Best parameters for Random Forest:\", rf_grid.best_params_)\nrf_pred = rf_grid.predict(X_test)\nrf_pred_prob = rf_grid.predict_proba(X_test)[:, 1]\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:29:02.596375Z","iopub.execute_input":"2024-10-14T10:29:02.597029Z","iopub.status.idle":"2024-10-14T10:33:20.864725Z","shell.execute_reply.started":"2024-10-14T10:29:02.596986Z","shell.execute_reply":"2024-10-14T10:33:20.863592Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 36 candidates, totalling 180 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Best parameters for Random Forest: {'classifier__max_depth': None, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 200}\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    0.9840    0.9904       313\n           1     0.9836    0.9967    0.9901       300\n\n    accuracy                         0.9902       613\n   macro avg     0.9902    0.9903    0.9902       613\nweighted avg     0.9903    0.9902    0.9902       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for SVM\nsvm_params = {\n    'classifier__C': [0.1, 1, 10],\n    'classifier__kernel': ['linear', 'rbf']\n}\n# Perform Grid Search CV for SVM\nsvm_grid = GridSearchCV(svm_pipeline, svm_params, cv=5, n_jobs=-1, verbose=1)\nsvm_grid.fit(X_train, y_train)\nprint(\"Best parameters for SVM:\", svm_grid.best_params_)\nsvm_pred = svm_grid.predict(X_test)\nsvm_pred_prob = svm_grid.predict_proba(X_test)[:, 1]\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_pred, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:33:20.866005Z","iopub.execute_input":"2024-10-14T10:33:20.866326Z","iopub.status.idle":"2024-10-14T10:35:22.324851Z","shell.execute_reply.started":"2024-10-14T10:33:20.866291Z","shell.execute_reply":"2024-10-14T10:35:22.323870Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 6 candidates, totalling 30 fits\nBest parameters for SVM: {'classifier__C': 1, 'classifier__kernel': 'linear'}\nSVM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9968    0.9984       313\n           1     0.9967    1.0000    0.9983       300\n\n    accuracy                         0.9984       613\n   macro avg     0.9983    0.9984    0.9984       613\nweighted avg     0.9984    0.9984    0.9984       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for KNN\nknn_params = {\n    'classifier__n_neighbors': [3, 5, 7],\n    'classifier__weights': ['uniform', 'distance']\n}\n# Perform Grid Search CV for KNN\nknn_grid = GridSearchCV(knn_pipeline, knn_params, cv=5, n_jobs=-1, verbose=1)\nknn_grid.fit(X_train, y_train)\nprint(\"Best parameters for KNN:\", knn_grid.best_params_)\nknn_pred = knn_grid.predict(X_test)\nknn_pred_prob = knn_grid.predict_proba(X_test)[:, 1]\nprint(\"KNN Classification Report:\")\nprint(classification_report(y_test, knn_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:35:22.326367Z","iopub.execute_input":"2024-10-14T10:35:22.326702Z","iopub.status.idle":"2024-10-14T10:37:07.100636Z","shell.execute_reply.started":"2024-10-14T10:35:22.326640Z","shell.execute_reply":"2024-10-14T10:37:07.099800Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 6 candidates, totalling 30 fits\nBest parameters for KNN: {'classifier__n_neighbors': 3, 'classifier__weights': 'distance'}\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9617    0.9805       313\n           1     0.9615    1.0000    0.9804       300\n\n    accuracy                         0.9804       613\n   macro avg     0.9808    0.9808    0.9804       613\nweighted avg     0.9812    0.9804    0.9804       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save Traditional ML Models","metadata":{}},{"cell_type":"code","source":"# Save traditional ML models\nif not os.path.exists('models'):\n    os.makedirs('models')\n\nwith open('models/rf_pipeline.pkl', 'wb') as f:\n    pickle.dump(rf_grid.best_estimator_, f)\n\nwith open('models/svm_pipeline.pkl', 'wb') as f:\n    pickle.dump(svm_grid.best_estimator_, f)\n\nwith open('models/knn_pipeline.pkl', 'wb') as f:\n    pickle.dump(knn_grid.best_estimator_, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:37:07.103142Z","iopub.execute_input":"2024-10-14T10:37:07.103453Z","iopub.status.idle":"2024-10-14T10:37:07.241598Z","shell.execute_reply.started":"2024-10-14T10:37:07.103420Z","shell.execute_reply":"2024-10-14T10:37:07.240819Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Models","metadata":{}},{"cell_type":"code","source":"\n# Tokenize and pad sequences for deep learning models\nmax_words = 10000\nmax_len = 500\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n\ndef build_cnn_model(hp):\n    model = Sequential()\n    model.add(Embedding(max_words, 128, input_length=max_len))\n    model.add(Conv1D(\n        filters=hp.Int('filters', min_value=32, max_value=256, step=32),\n        kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]),\n        activation='relu'\n    ))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(\n        units=hp.Int('units', min_value=32, max_value=256, step=32),\n        activation='relu'\n    ))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\ndef build_lstm_model(hp):\n    model = Sequential()\n    model.add(Embedding(max_words, 128, input_length=max_len))\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(\n        units=hp.Int('lstm_units', min_value=32, max_value=256, step=32),\n        dropout=0.2,\n        recurrent_dropout=0.2\n    ))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:37:07.242762Z","iopub.execute_input":"2024-10-14T10:37:07.243074Z","iopub.status.idle":"2024-10-14T10:37:09.261128Z","shell.execute_reply.started":"2024-10-14T10:37:07.243042Z","shell.execute_reply":"2024-10-14T10:37:09.260153Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for CNN\ncnn_tuner = kt.Hyperband(\n    build_cnn_model,\n    objective='val_accuracy',\n    max_epochs=10,\n    hyperband_iterations=2,\n    directory='my_dir',\n    project_name='cnn_tuning'\n)\n\n\n\n# Search for best hyperparameters\ncnn_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n\n# Retrieve best hyperparameters for CNN\nbest_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search for CNN is complete. The optimal number of filters in the Conv1D layer is {best_cnn_hps.get('filters')},\nthe optimal kernel size is {best_cnn_hps.get('kernel_size')}, and the optimal number of units in the Dense layer is {best_cnn_hps.get('units')}.\n\"\"\")\n\n\n# Train with best hyperparameters for CNN\ncnn_model = cnn_tuner.hypermodel.build(best_cnn_hps)\ncnn_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\ncnn_model.save('models/best_cnn_model.h5')\n\ncnn_pred_prob = cnn_model.predict(X_test_pad)\ncnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\nprint(\"CNN Classification Report:\")\nprint(classification_report(y_test, cnn_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:37:09.262350Z","iopub.execute_input":"2024-10-14T10:37:09.262679Z","iopub.status.idle":"2024-10-14T10:43:03.164607Z","shell.execute_reply.started":"2024-10-14T10:37:09.262632Z","shell.execute_reply":"2024-10-14T10:43:03.163623Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Trial 60 Complete [00h 00m 08s]\nval_accuracy: 0.9959266781806946\n\nBest val_accuracy So Far: 0.9979633688926697\nTotal elapsed time: 00h 05m 46s\n\nThe hyperparameter search for CNN is complete. The optimal number of filters in the Conv1D layer is 64,\nthe optimal kernel size is 7, and the optimal number of units in the Dense layer is 96.\n\nEpoch 1/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7545 - loss: 0.5959 - val_accuracy: 0.9817 - val_loss: 0.0691\nEpoch 2/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0401 - val_accuracy: 0.9878 - val_loss: 0.0315\nEpoch 3/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.0054 - val_accuracy: 0.9857 - val_loss: 0.0277\nEpoch 4/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9898 - val_loss: 0.0252\nEpoch 5/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 9.3815e-04 - val_accuracy: 0.9898 - val_loss: 0.0236\nEpoch 6/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.9164e-04 - val_accuracy: 0.9919 - val_loss: 0.0231\nEpoch 7/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.4461e-04 - val_accuracy: 0.9898 - val_loss: 0.0227\nEpoch 8/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.3416e-04 - val_accuracy: 0.9898 - val_loss: 0.0224\nEpoch 9/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0439e-04 - val_accuracy: 0.9919 - val_loss: 0.0225\nEpoch 10/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.5421e-04 - val_accuracy: 0.9919 - val_loss: 0.0225\n\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\nCNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9935    0.9840    0.9888       313\n           1     0.9835    0.9933    0.9884       300\n\n    accuracy                         0.9886       613\n   macro avg     0.9885    0.9887    0.9886       613\nweighted avg     0.9886    0.9886    0.9886       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for LSTM\nlstm_tuner = kt.Hyperband(\n    build_lstm_model,\n    objective='val_accuracy',\n    max_epochs=10,\n    hyperband_iterations=2,\n    directory='my_dir',\n    project_name='lstm_tuning'\n)\n\n# Search for best hyperparameters\nlstm_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n\n# Retrieve best hyperparameters for LSTM\nbest_lstm_hps = lstm_tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is {best_lstm_hps.get('lstm_units')}.\n\"\"\")\n\n# Train with best hyperparameters for LSTM\nlstm_model = lstm_tuner.hypermodel.build(best_lstm_hps)\nlstm_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\nlstm_model.save('models/best_lstm_model.h5')\n\nlstm_pred_prob = lstm_model.predict(X_test_pad)\nlstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test, lstm_pred, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T10:43:03.167192Z","iopub.execute_input":"2024-10-14T10:43:03.167478Z","iopub.status.idle":"2024-10-14T11:03:04.197742Z","shell.execute_reply.started":"2024-10-14T10:43:03.167447Z","shell.execute_reply":"2024-10-14T11:03:04.196799Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Trial 8 Complete [00h 01m 37s]\nval_accuracy: 0.9857434034347534\n\nBest val_accuracy So Far: 0.9918533563613892\nTotal elapsed time: 00h 12m 31s\n\nThe hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is 192.\n\nEpoch 1/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 717ms/step - accuracy: 0.7315 - loss: 0.5346 - val_accuracy: 0.9796 - val_loss: 0.1445\nEpoch 2/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 720ms/step - accuracy: 0.9660 - loss: 0.1782 - val_accuracy: 0.9817 - val_loss: 0.0557\nEpoch 3/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 723ms/step - accuracy: 0.9891 - loss: 0.0374 - val_accuracy: 0.9735 - val_loss: 0.0898\nEpoch 4/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 717ms/step - accuracy: 0.9825 - loss: 0.0465 - val_accuracy: 0.9878 - val_loss: 0.0487\nEpoch 5/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 716ms/step - accuracy: 0.9945 - loss: 0.0208 - val_accuracy: 0.9776 - val_loss: 0.0609\nEpoch 6/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 719ms/step - accuracy: 0.9984 - loss: 0.0129 - val_accuracy: 0.9817 - val_loss: 0.0628\nEpoch 7/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 714ms/step - accuracy: 0.9973 - loss: 0.0090 - val_accuracy: 0.9878 - val_loss: 0.0438\nEpoch 8/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 717ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9919 - val_loss: 0.0425\nEpoch 9/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 711ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9837 - val_loss: 0.0672\nEpoch 10/10\n\u001b[1m62/62\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 712ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9857 - val_loss: 0.0559\n\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 185ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    0.9936    0.9952       313\n           1     0.9934    0.9967    0.9950       300\n\n    accuracy                         0.9951       613\n   macro avg     0.9951    0.9951    0.9951       613\nweighted avg     0.9951    0.9951    0.9951       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LMMs","metadata":{}},{"cell_type":"code","source":"def train_hf_model(model_name, tokenizer_class, model_class, X_train, y_train, X_test, y_test, save_path):\n    os.environ['WANDB_DISABLED'] = 'true'\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n\n    train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512)\n    test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512)\n\n    # Convert the data into Hugging Face Dataset format\n    train_dataset = HFDataset.from_dict({\n        'input_ids': train_encodings['input_ids'], \n        'attention_mask': train_encodings['attention_mask'], \n        'labels': y_train.tolist()\n    })\n    \n    test_dataset = HFDataset.from_dict({\n        'input_ids': test_encodings['input_ids'], \n        'attention_mask': test_encodings['attention_mask'], \n        'labels': y_test.tolist()\n    })\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n        load_best_model_at_end=True,  # Load best model during evaluation\n        save_strategy=\"epoch\",  # Save at the end of each epoch\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        tokenizer=tokenizer,\n    )\n\n    print(f\"Training {model_name} model...\")\n    \n    # Adding progress bar\n    from tqdm.auto import tqdm\n    tqdm.pandas()\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    predictions = trainer.predict(test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n    print(f\"{model_name} Classification Report:\")\n    print(classification_report(y_test, preds, digits=4))\n\n    # Save the trained model and tokenizer\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:08:16.479458Z","iopub.execute_input":"2024-10-14T11:08:16.480161Z","iopub.status.idle":"2024-10-14T11:08:16.491801Z","shell.execute_reply.started":"2024-10-14T11:08:16.480121Z","shell.execute_reply":"2024-10-14T11:08:16.490958Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"# Train and save BERT model\ntrain_hf_model(\n    model_name='bert-base-uncased',\n    tokenizer_class=BertTokenizer,\n    model_class=BertForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/bert_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T06:40:21.253189Z","iopub.execute_input":"2024-10-13T06:40:21.253730Z","iopub.status.idle":"2024-10-13T06:50:54.587183Z","shell.execute_reply.started":"2024-10-13T06:40:21.253678Z","shell.execute_reply":"2024-10-13T06:50:54.586309Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training bert-base-uncased model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 07:34, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.274600</td>\n      <td>0.154265</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364700</td>\n      <td>0.059009</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000200</td>\n      <td>0.050542</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"bert-base-uncased Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9936    0.9904    0.9920       313\n           1     0.9900    0.9933    0.9917       300\n\n    accuracy                         0.9918       613\n   macro avg     0.9918    0.9919    0.9918       613\nweighted avg     0.9918    0.9918    0.9918       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"# Train and save DistilBERT model\ntrain_hf_model(\n    model_name='distilbert-base-uncased',\n    tokenizer_class=DistilBertTokenizer,\n    model_class=DistilBertForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/distilbert_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T06:50:54.588932Z","iopub.execute_input":"2024-10-13T06:50:54.589311Z","iopub.status.idle":"2024-10-13T06:57:46.853116Z","shell.execute_reply.started":"2024-10-13T06:50:54.589267Z","shell.execute_reply":"2024-10-13T06:57:46.851943Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fccbc9b94749eeaf5b3f71e4354f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8428b2536a1140afb7991630ead97412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b5b83ce6394193a60dc98b3ed8fa91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4b36e887454de9b95fefc68f394411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73c4123c78a4b8fa0af2effbc773650"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training distilbert-base-uncased model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 03:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.076900</td>\n      <td>0.102859</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.146000</td>\n      <td>0.056145</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000400</td>\n      <td>0.067037</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"distilbert-base-uncased Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9936    0.9904    0.9920       313\n           1     0.9900    0.9933    0.9917       300\n\n    accuracy                         0.9918       613\n   macro avg     0.9918    0.9919    0.9918       613\nweighted avg     0.9918    0.9918    0.9918       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"# Train and save RoBERTa model\ntrain_hf_model(\n    model_name='roberta-base',\n    tokenizer_class=RobertaTokenizer,\n    model_class=RobertaForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/roberta_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:09:35.275975Z","iopub.execute_input":"2024-10-14T11:09:35.276765Z","iopub.status.idle":"2024-10-14T11:18:13.379389Z","shell.execute_reply.started":"2024-10-14T11:09:35.276722Z","shell.execute_reply":"2024-10-14T11:18:13.378576Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94fb57bf310f4705b25960002572748e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3bf569e30464c828070d6fbcb962064"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb99f8b4e7e4d069860958934459577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc055608fdd44181817eeaf718830408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7de0b95b634df9aad8eab784cefa3c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2f734dff5a488ea24c6ee857ec80d4"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training roberta-base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 07:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.185000</td>\n      <td>0.069999</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.263600</td>\n      <td>0.185212</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.087300</td>\n      <td>0.068845</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"roberta-base Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9935    0.9840    0.9888       313\n           1     0.9835    0.9933    0.9884       300\n\n    accuracy                         0.9886       613\n   macro avg     0.9885    0.9887    0.9886       613\nweighted avg     0.9886    0.9886    0.9886       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT Large","metadata":{}},{"cell_type":"code","source":"# # Train and save BERT Large model\n# train_hf_model(\n#     model_name='bert-large-uncased',\n#     tokenizer_class=BertTokenizer,\n#     model_class=BertForSequenceClassification,\n#     X_train=X_train,\n#     y_train=y_train,\n#     X_test=X_test,\n#     y_test=y_test,\n#     save_path='models/bert_large_model'\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:18:13.381264Z","iopub.execute_input":"2024-10-14T11:18:13.381604Z","iopub.status.idle":"2024-10-14T11:18:13.385759Z","shell.execute_reply.started":"2024-10-14T11:18:13.381569Z","shell.execute_reply":"2024-10-14T11:18:13.384873Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Plot ROC curves\ndef plot_roc_curve(y_true, y_pred_prob, model_name):\n    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'Receiver Operating Characteristic - {model_name}')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    print(f'{model_name} AUROC: {roc_auc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:18:13.386783Z","iopub.execute_input":"2024-10-14T11:18:13.387036Z","iopub.status.idle":"2024-10-14T11:18:13.401486Z","shell.execute_reply.started":"2024-10-14T11:18:13.387008Z","shell.execute_reply":"2024-10-14T11:18:13.400506Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n# Download NLTK data files (if not already installed)\nnltk.download('stopwords')\nnltk.download('punkt')\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef clean_text(text, stop_words):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n    text = text.lower()\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return ' '.join(filtered_text)\n\ndef stem_text(text):\n    stemmer = PorterStemmer()\n    word_tokens = word_tokenize(text)\n    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n    return ' '.join(stemmed_text)\n\ndef ml_preprocess(df):\n    stop_words = set(stopwords.words('english'))\n    df['text'] = df['text'].apply(lambda x: clean_text(x, stop_words))\n    df['text'] = df['text'].apply(stem_text)\n    return df\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/phishing-email-dataset-nazario-5-and-trec07/email_text.csv\")  # Update with the actual path\n\n# Preprocess the dataset\ndf = ml_preprocess(df)\n\n# Split the dataset into samples where the label is 1 (spam emails)\ndf_label_1 = df[df['label'] == 1]\n\n# Change the label of these samples to 0\ndf_label_1['label'] = 0\n\n# Define the data for testing\nX_test = df_label_1['text']\ny_test = df_label_1['label']\n\n# Tokenize and pad sequences for deep learning models\nmax_words = 10000\nmax_len = 500\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_test)\n\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:18:13.404736Z","iopub.execute_input":"2024-10-14T11:18:13.405016Z","iopub.status.idle":"2024-10-14T11:27:35.520678Z","shell.execute_reply.started":"2024-10-14T11:18:13.404986Z","shell.execute_reply":"2024-10-14T11:27:35.519772Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2491548583.py:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_label_1['label'] = 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load traditional ML models\nwith open('/kaggle/working/models/rf_pipeline.pkl', 'rb') as f:\n    rf_model = pickle.load(f)\n\nwith open('/kaggle/working/models/svm_pipeline.pkl', 'rb') as f:\n    svm_model = pickle.load(f)\n\nwith open('/kaggle/working/models/knn_pipeline.pkl', 'rb') as f:\n    knn_model = pickle.load(f)\n    \n# Predict and print classification report and ROC curve for Random Forest\nrf_pred = rf_model.predict(X_test)\nrf_pred_prob = rf_model.predict_proba(X_test)[:, 1]\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred, digits=4))\n\n# Predict and print classification report and ROC curve for SVM\nsvm_pred = svm_model.predict(X_test)\nsvm_pred_prob = svm_model.predict_proba(X_test)[:, 1]\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_pred, digits=4))\n\n# Predict and print classification report and ROC curve for KNN\nknn_pred = knn_model.predict(X_test)\nknn_pred_prob = knn_model.predict_proba(X_test)[:, 1]\nprint(\"KNN Classification Report:\")\nprint(classification_report(y_test, knn_pred, digits=4))\n\n\n\n\n\n\n\n# # Plot ROC curves for all models\n# plot_roc_curve(y_test, rf_pred_prob, 'Random Forest')\n# plot_roc_curve(y_test, svm_pred_prob, 'SVM')\n# plot_roc_curve(y_test, knn_pred_prob, 'KNN')\n# plot_roc_curve(y_test, cnn_pred_prob, 'CNN')\n# plot_roc_curve(y_test, lstm_pred_prob, 'LSTM')\n# plot_roc_curve(y_test, bert_pred_prob, 'BERT')\n# plot_roc_curve(y_test, distilbert_pred_prob, 'DistilBERT')\n# plot_roc_curve(y_test, roberta_pred_prob, 'RoBERTa')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:27:35.522971Z","iopub.execute_input":"2024-10-14T11:27:35.523293Z","iopub.status.idle":"2024-10-14T11:34:49.641017Z","shell.execute_reply.started":"2024-10-14T11:27:35.523254Z","shell.execute_reply":"2024-10-14T11:34:49.639877Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Random Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7085    0.8294     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7085     29923\n   macro avg     0.5000    0.3542    0.4147     29923\nweighted avg     1.0000    0.7085    0.8294     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"SVM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9174    0.9569     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.9174     29923\n   macro avg     0.5000    0.4587    0.4785     29923\nweighted avg     1.0000    0.9174    0.9569     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"KNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.6269    0.7707     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.6269     29923\n   macro avg     0.5000    0.3135    0.3854     29923\nweighted avg     1.0000    0.6269    0.7707     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load deep learning models\ncnn_model = load_model('/kaggle/working/models/best_cnn_model.h5')\nlstm_model = load_model('/kaggle/working/models/best_lstm_model.h5')\n\n# Predict and print classification report and ROC curve for CNN\ncnn_pred_prob = cnn_model.predict(X_test_pad)\ncnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\nprint(\"CNN Classification Report:\")\nprint(classification_report(y_test, cnn_pred, digits=4))\n\n# Predict and print classification report and ROC curve for LSTM\nlstm_pred_prob = lstm_model.predict(X_test_pad)\nlstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test, lstm_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:34:49.642508Z","iopub.execute_input":"2024-10-14T11:34:49.642965Z","iopub.status.idle":"2024-10-14T11:37:41.576465Z","shell.execute_reply.started":"2024-10-14T11:34:49.642910Z","shell.execute_reply":"2024-10-14T11:37:41.575601Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728905690.794776     125 service.cc:145] XLA service 0x7f6eec003470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1728905690.794836     125 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m118/936\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1728905691.533103     125 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m936/936\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\nCNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7737    0.8724     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7737     29923\n   macro avg     0.5000    0.3868    0.4362     29923\nweighted avg     1.0000    0.7737    0.8724     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m936/936\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 178ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.8476    0.9175     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8476     29923\n   macro avg     0.5000    0.4238    0.4588     29923\nweighted avg     1.0000    0.8476    0.9175     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_hf_model(model, tokenizer, X_test):\n    encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n    outputs = model(**encodings)\n    predictions = torch.argmax(outputs.logits, dim=1).detach().numpy()\n    pred_prob = torch.softmax(outputs.logits, dim=1).detach().numpy()[:, 1]\n    return predictions, pred_prob\n\ndef predict_hf_model_batch(model, tokenizer, dataloader, device):\n    model = model.to(device)\n    model.eval()\n    all_preds = []\n    all_probs = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Predicting\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n            all_preds.extend(preds)\n            all_probs.extend(probs)\n    return np.array(all_preds), np.array(all_probs)\n# Parameters\nBATCH_SIZE = 64\nMAX_LENGTH = 512\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:37:41.577754Z","iopub.execute_input":"2024-10-14T11:37:41.578064Z","iopub.status.idle":"2024-10-14T11:37:41.592069Z","shell.execute_reply.started":"2024-10-14T11:37:41.578031Z","shell.execute_reply":"2024-10-14T11:37:41.591231Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Load Hugging Face models using safetensors\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/working/models/bert_model')\nbert_model = BertForSequenceClassification.from_pretrained('/kaggle/working/models/bert_model', use_safetensors=True)\n\n# Create DataLoader\ntest_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), bert_tokenizer, MAX_LENGTH)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n\n# Predict and print classification report and ROC curve for BERT\nbert_pred, bert_pred_prob = predict_hf_model_batch(bert_model, bert_tokenizer, test_dataloader, device)\nprint(\"BERT Classification Report:\")\nprint(classification_report(y_test, bert_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:37:41.593458Z","iopub.execute_input":"2024-10-14T11:37:41.594231Z","iopub.status.idle":"2024-10-14T11:48:37.843858Z","shell.execute_reply.started":"2024-10-14T11:37:41.594188Z","shell.execute_reply":"2024-10-14T11:48:37.842896Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 468/468 [10:55<00:00,  1.40s/batch]","output_type":"stream"},{"name":"stdout","text":"BERT Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.8427    0.9147     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8427     29923\n   macro avg     0.5000    0.4214    0.4573     29923\nweighted avg     1.0000    0.8427    0.9147     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('/kaggle/working/models/distilbert_model')\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('/kaggle/working/models/distilbert_model', use_safetensors=True)\n\n# Predict and print classification report and ROC curve for DistilBERT\ndistilbert_test_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), distilbert_tokenizer, MAX_LENGTH)\ndistilbert_test_dataloader = DataLoader(distilbert_test_dataset, batch_size=BATCH_SIZE)\n\ndistilbert_pred, distilbert_pred_prob = predict_hf_model_batch(distilbert_model, distilbert_tokenizer, distilbert_test_dataloader, device)\nprint(\"DistilBERT Classification Report:\")\nprint(classification_report(y_test, distilbert_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:48:37.845114Z","iopub.execute_input":"2024-10-14T11:48:37.845411Z","iopub.status.idle":"2024-10-14T11:56:12.116899Z","shell.execute_reply.started":"2024-10-14T11:48:37.845379Z","shell.execute_reply":"2024-10-14T11:56:12.115975Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 468/468 [07:34<00:00,  1.03batch/s]","output_type":"stream"},{"name":"stdout","text":"DistilBERT Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7822    0.8778     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7822     29923\n   macro avg     0.5000    0.3911    0.4389     29923\nweighted avg     1.0000    0.7822    0.8778     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"roberta_tokenizer = RobertaTokenizer.from_pretrained('/kaggle/working/models/roberta_model')\nroberta_model = RobertaForSequenceClassification.from_pretrained('models/roberta_model', use_safetensors=True)\n\n# Predict and print classification report and ROC curve for RoBERTa\nroberta_test_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), roberta_tokenizer, MAX_LENGTH)\nroberta_test_dataloader = DataLoader(roberta_test_dataset, batch_size=BATCH_SIZE)\n\nroberta_pred, roberta_pred_prob = predict_hf_model_batch(roberta_model, roberta_tokenizer, roberta_test_dataloader, device)\nprint(\"RoBERTa Classification Report:\")\nprint(classification_report(y_test, roberta_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:56:12.118137Z","iopub.execute_input":"2024-10-14T11:56:12.118455Z","iopub.status.idle":"2024-10-14T12:04:52.829367Z","shell.execute_reply.started":"2024-10-14T11:56:12.118421Z","shell.execute_reply":"2024-10-14T12:04:52.828320Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 468/468 [08:40<00:00,  1.11s/batch]","output_type":"stream"},{"name":"stdout","text":"RoBERTa Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7738    0.8725     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7738     29923\n   macro avg     0.5000    0.3869    0.4362     29923\nweighted avg     1.0000    0.7738    0.8725     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}