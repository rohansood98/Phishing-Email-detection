{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9106973,"sourceType":"datasetVersion","datasetId":5496271},{"sourceId":9489801,"sourceType":"datasetVersion","datasetId":5773614}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n%pip install -U transformers accelerate bitsandbytes peft trl datasets --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-27T10:03:26.341096Z","iopub.execute_input":"2024-09-27T10:03:26.341368Z","iopub.status.idle":"2024-09-27T10:04:03.935739Z","shell.execute_reply.started":"2024-09-27T10:03:26.341338Z","shell.execute_reply":"2024-09-27T10:04:03.934388Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport re\nimport string\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, Trainer\nfrom tqdm import tqdm\nimport huggingface_hub\n\n#Specify a read token\nhf_token = \"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\"\nos.environ[\"HUGGINGFACE_TOKEN\"] = hf_token\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nhuggingface_hub.login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T10:04:03.938248Z","iopub.execute_input":"2024-09-27T10:04:03.938689Z","iopub.status.idle":"2024-09-27T10:04:34.687685Z","shell.execute_reply.started":"2024-09-27T10:04:03.938639Z","shell.execute_reply":"2024-09-27T10:04:34.686767Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Read the base model and Tokenizer.","metadata":{}},{"cell_type":"code","source":"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token, trust_remote_code=True)\ntokenizer.add_bos_token = True\ntokenizer.add_eos_token = True\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    use_auth_token=hf_token\n)\n\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2024-09-27T10:04:34.689149Z","iopub.execute_input":"2024-09-27T10:04:34.689760Z","iopub.status.idle":"2024-09-27T10:04:35.621179Z","shell.execute_reply.started":"2024-09-27T10:04:34.689720Z","shell.execute_reply":"2024-09-27T10:04:35.620303Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16e6c96174b4a2cbb6e39e7e490c1a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da659104a6474d21ac5e2e23edf8d879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba4c8aa0846e42d39780077650c116da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005b6c499feb45b2a1e0109f8d3ca3f0"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Data PreProcessing","metadata":{}},{"cell_type":"code","source":"import random\ndef clean_text(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n    text = text.lower()\n    return text\n\ndef ml_preprocess(df):\n    df['subject'] = df['subject'].fillna('')\n    df['content'] = df['subject'] + ' ' + df['body']\n    df['content'] = df['content'].apply(clean_text)\n    df = df[['label', 'content']]\n    return df\n\ndef ml_preprocess_2(df):\n    df['content'] = df['text'].apply(clean_text)\n    df = df[['label', 'content']].dropna()\n    return df\nmax_seq_length = 1024\n\ndef truncate_content(content, max_length=600):\n    return content[:max_length] + \"...\" if len(content) > max_length else content\n\n# Updated format_dataset_with_prompt function\ndef format_dataset_with_prompt(examples):\n    prompt_template = \"\"\"\n[INST] You are a phishing detection classifier. Classify the email as phishing (1) or non-phishing (0).\nReturn ONLY the integer 1 or 0. Do not provide any explanation or additional text. Classify only phishing emails as phishing (1) and spam and non-phishing as non-phishing (0)\n### Example 1:\nEmail Content:\n\"Hi how is everyone\"\nYour Response: 0\n### Example 2:\nEmail Content:\n\"Your account has been hacked click the link below URLFOUND\"\nYour Response: 1\n### Now classify this email:\nEmail Content:\n\"{content}\"\nYour Response: [/INST]\n\"\"\"\n    prompts = []\n    responses = []\n    for content, label in zip(examples['content'], examples['label']):\n        prompt = prompt_template.format(content=truncate_content(content))\n        response = str(label)\n        prompts.append(prompt)\n        responses.append(response)\n    return {\"prompt\": prompts, \"response\": responses}\n\n# Load and preprocess dataset\ndf = pd.read_csv(\"/kaggle/input/my-phishing-dataset/Nazario_5.csv\")\ndf = ml_preprocess(df)\n\nspam_samples = pd.read_csv(\"/kaggle/input/my-phishing-dataset/email_text.csv\")\n\n# Split the dataset into samples where the label is 1 (phishing emails)\nspam_samples = spam_samples[spam_samples['label'] == 1].copy()\n\n# Change the label of these samples to 0 (as per your original code)\nspam_samples['label'] = 0\nspam_samples =spam_samples[:30]\nspam_samples = ml_preprocess_2(spam_samples)\n\n\n# Split the data into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df = pd.concat([train_df, spam_samples], ignore_index=True)\n\n# train_df=train_df[:400]\ntrain_df = pd.concat([train_df, spam_samples], ignore_index=True)\n\n\n# Convert to Hugging Face datasets\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n\n# Apply formatting to datasets\ntrain_dataset = train_dataset.map(\n    format_dataset_with_prompt,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)\ntest_dataset = test_dataset.map(\n    format_dataset_with_prompt,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\n\n# Define custom data collator\nclass DataCollatorForSupervisedDataset:\n    def __init__(self, tokenizer, max_seq_length=1024):\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __call__(self, instances):\n        input_ids_list = []\n        labels_list = []\n\n        for instance in instances:\n            prompt = instance['prompt']\n            response = instance['response']\n\n            # Tokenize prompt and response separately\n            prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n            response_ids = self.tokenizer.encode(response, add_special_tokens=False)\n\n            # Concatenate prompt and response\n            input_ids = prompt_ids + response_ids + [self.tokenizer.eos_token_id]\n\n            # Create labels: -100 for prompt tokens, actual ids for response tokens\n            labels = [-100] * len(prompt_ids) + response_ids + [self.tokenizer.eos_token_id]\n\n            # Truncate if necessary\n            if len(input_ids) > self.max_seq_length:\n                input_ids = input_ids[:self.max_seq_length]\n                labels = labels[:self.max_seq_length]\n\n            input_ids_list.append(torch.tensor(input_ids))\n            labels_list.append(torch.tensor(labels))\n\n        # Pad sequences\n        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n            input_ids_list,\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id\n        )\n        labels_padded = torch.nn.utils.rnn.pad_sequence(\n            labels_list,\n            batch_first=True,\n            padding_value=-100\n        )\n        attention_mask = (input_ids_padded != self.tokenizer.pad_token_id).long()\n\n        return {\n            \"input_ids\": input_ids_padded,\n            \"labels\": labels_padded,\n            \"attention_mask\": attention_mask\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T10:06:49.206198Z","iopub.execute_input":"2024-09-27T10:06:49.206599Z","iopub.status.idle":"2024-09-27T10:06:53.700156Z","shell.execute_reply.started":"2024-09-27T10:06:49.206559Z","shell.execute_reply":"2024-09-27T10:06:53.699275Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab87f14f73a4b1db7e2820554edac83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2529c6aaeec45d586216855fc654366"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"ffn\", \"gate_proj\"],\n)\n\nmodel = get_peft_model(model, peft_config)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=20,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    save_strategy=\"steps\",\n    remove_unused_columns=False,\n)\n\n# Define data collator\ndata_collator = DataCollatorForSupervisedDataset(\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length\n)\n\n# Create SFT Trainer with the custom data collator\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    dataset_text_field=\"prompt\"\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T04:01:33.014144Z","iopub.execute_input":"2024-09-27T04:01:33.014449Z","iopub.status.idle":"2024-09-27T06:14:16.906325Z","shell.execute_reply.started":"2024-09-27T04:01:33.014416Z","shell.execute_reply":"2024-09-27T06:14:16.905327Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:576: UserWarning: You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to inspect dataset other columns (in this case ['prompt', 'response']), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c57960050f9c4dd8bed0c39b4871a359"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [471/471 2:11:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.925000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.653500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.330600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.170200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.029600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.044600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.076900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.061400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.114500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.032500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.083200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.030700</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.005700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.030400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.011800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=471, training_loss=0.10357647573115625, metrics={'train_runtime': 7960.1486, 'train_samples_per_second': 0.947, 'train_steps_per_second': 0.059, 'total_flos': 9.973843107014246e+16, 'train_loss': 0.10357647573115625, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save the Model","metadata":{}},{"cell_type":"code","source":"\n\n# Merge and save the fine-tuned model\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./mistral_finetuned_merged\")\ntokenizer.save_pretrained(\"./mistral_finetuned_merged\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:14:16.907575Z","iopub.execute_input":"2024-09-27T06:14:16.907891Z","iopub.status.idle":"2024-09-27T06:14:56.112132Z","shell.execute_reply.started":"2024-09-27T06:14:16.907856Z","shell.execute_reply":"2024-09-27T06:14:56.110981Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('./mistral_finetuned_merged/tokenizer_config.json',\n './mistral_finetuned_merged/special_tokens_map.json',\n './mistral_finetuned_merged/tokenizer.model',\n './mistral_finetuned_merged/added_tokens.json',\n './mistral_finetuned_merged/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Saved Model if needed","metadata":{}},{"cell_type":"code","source":"\n\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"./mistral_finetuned_merged\",\n#     device_map=\"auto\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n\n# model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n# model.eval()\n\n# tokenizer = AutoTokenizer.from_pretrained(\"./mistral_finetuned_merged\")\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n# model.config.pad_token_id = model.config.eos_token_id\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:14:56.123316Z","iopub.execute_input":"2024-09-27T06:14:56.123729Z","iopub.status.idle":"2024-09-27T06:14:56.129758Z","shell.execute_reply.started":"2024-09-27T06:14:56.123693Z","shell.execute_reply":"2024-09-27T06:14:56.128537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Sample Test Email","metadata":{}},{"cell_type":"code","source":"model.eval()\n\n# Update the standardize_output function\ndef standardize_output(result):\n    result = result.strip()\n    if '1' in result:\n        return 1\n    elif '0' in result:\n        return 0\n    else:\n        return -1  # Invalid output\n\n# Test the fine-tuned model with a sample email\ntest_email = (\n    \"This is ROhan Sood\"\n)\n\n# Prepare the prompt\nprompt_template = \"\"\"\n[INST] You are a phishing detection classifier. Classify the email as phishing (1) or non-phishing (0). Classify only phishing emails as phishing (1) and spam and non-phishing as non-phishing (0)\nReturn ONLY the integer 1 or 0. Do not provide any explanation or additional text.\n### Example 1:\nEmail Content:\n\"Hi how is everyone\"\nYour Response: 0\n### Example 2:\nEmail Content:\n\"Your account has been hacked click the link below URLFOUND\"\nYour Response: 1\n### Now classify this email:\nEmail Content:\n\"{content}\"\nYour Response: [/INST]\n\"\"\"\nprompt = prompt_template.format(content=test_email)\n\n# Tokenize the prompt\nsample_input = tokenizer(\n    prompt,\n    return_tensors=\"pt\",\n    truncation=True,\n    max_length=max_seq_length\n).to(\"cuda\")\n\n# Generate the output\nwith torch.no_grad():\n    sample_output = model.generate(\n        **sample_input,\n        max_new_tokens=2,  # Limit the output length\n        do_sample=False,\n        num_beams=1,\n        use_cache=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n# Decode the output\nsample_prediction = tokenizer.decode(\n    sample_output[0, sample_input['input_ids'].shape[1]:],\n    skip_special_tokens=True\n)\nprint(f\"Sample email classification: {sample_prediction}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T10:11:54.389252Z","iopub.execute_input":"2024-09-27T10:11:54.389642Z","iopub.status.idle":"2024-09-27T10:11:55.019857Z","shell.execute_reply.started":"2024-09-27T10:11:54.389603Z","shell.execute_reply":"2024-09-27T10:11:55.018928Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Sample email classification: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate the model on the test set","metadata":{}},{"cell_type":"code","source":"predictions = []\nunstandardized_predictions = []\nbatch_size = 16\n\nfor i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Generating predictions\", unit=\"batch\"):\n    batch = test_dataset[i:i + batch_size]\n    batch_predictions = []\n    batch_unstandardized = []\n    for idx, sample in enumerate(batch['prompt']):\n        sample_input = tokenizer(\n            sample,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=max_seq_length\n        ).to(\"cuda\")\n        with torch.no_grad():\n            sample_output = model.generate(\n                **sample_input,\n                max_new_tokens=2,\n                do_sample=False,\n                num_beams=1,\n                use_cache=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        # Decode only the newly generated tokens\n        sample_prediction = tokenizer.decode(\n            sample_output[0, sample_input['input_ids'].shape[1]:],\n            skip_special_tokens=True\n        )\n        batch_unstandardized.append(sample_prediction)\n        batch_predictions.append(standardize_output(sample_prediction))\n    unstandardized_predictions.extend(batch_unstandardized)\n    predictions.extend(batch_predictions)\n#     torch.cuda.empty_cache()\n\n# print(\"Standardized predictions:\", predictions)\nprint(\"Unstandardized predictions:\", unstandardized_predictions)\n\n# Evaluate predictions\nvalid_predictions = [pred for pred in predictions if pred != -1]\ny_true = test_df['label'].tolist()[:len(valid_predictions)]\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_true, valid_predictions, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:14:58.626299Z","iopub.execute_input":"2024-09-27T06:14:58.627166Z","iopub.status.idle":"2024-09-27T06:47:48.126150Z","shell.execute_reply.started":"2024-09-27T06:14:58.627111Z","shell.execute_reply":"2024-09-27T06:47:48.125019Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Generating predictions: 100%|██████████| 39/39 [32:49<00:00, 50.50s/batch]","output_type":"stream"},{"name":"stdout","text":"Unstandardized predictions: ['0', '0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '0', '1', '1', '1', '1', '1', '0', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '0', '0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '1', '1', '0', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '1', '1', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '0', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '0', '0', '0', '1', '1', '0', '0', '1', '1', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '0', '1', '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '1']\nClassification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    1.0000    0.9984       313\n           1     1.0000    0.9967    0.9983       300\n\n    accuracy                         0.9984       613\n   macro avg     0.9984    0.9983    0.9984       613\nweighted avg     0.9984    0.9984    0.9984       613\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stage 2 Test model on TREC_07 dataset","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport pandas as pd\nimport re\nimport string\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:47:48.127850Z","iopub.execute_input":"2024-09-27T06:47:48.128194Z","iopub.status.idle":"2024-09-27T06:47:48.133017Z","shell.execute_reply.started":"2024-09-27T06:47:48.128161Z","shell.execute_reply":"2024-09-27T06:47:48.131954Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Load Model ","metadata":{}},{"cell_type":"code","source":"\n\n# # Load the trained model and tokenizer\n# model_path = \"./mistral_finetuned_merged\"\n\n# # Load tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\n#     model_path,\n#     trust_remote_code=True\n# )\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = 'right'\n\n\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"./mistral_finetuned_merged\",\n#     device_map=\"auto\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n\n\n# model.eval()  # Set model to evaluation mode","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:47:48.134189Z","iopub.execute_input":"2024-09-27T06:47:48.134503Z","iopub.status.idle":"2024-09-27T06:47:48.144869Z","shell.execute_reply.started":"2024-09-27T06:47:48.134463Z","shell.execute_reply":"2024-09-27T06:47:48.143887Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"\ndef ml_preprocess(df):\n    df['content'] = df['text'].apply(clean_text)\n    df = df[['label', 'content']].dropna()\n    return df\n\n# Load the dataset (update with the actual path)\ndf = pd.read_csv(\"/kaggle/input/my-phishing-dataset/email_text.csv\")\n\n# Preprocess the dataset\ndf = ml_preprocess(df)\n\n# Split the dataset into samples where the label is 1 (phishing emails)\ndf_label_1 = df[df['label'] == 1].copy()\n\n# Change the label of these samples to 0 (as per your original code)\ndf_label_1['label'] = 0\n\ndf_label_1=df_label_1[30:1030]\n# Define the data for testing\nX_test = df_label_1['content'].tolist()\ny_test = df_label_1['label'].tolist()\n\n# Define the prompt template and helper functions\nprompt_template = \"\"\"\n[INST] You are a phishing detection classifier. Classify the email as phishing (1) or non-phishing (0). Classify only phishing emails as phishing (1) and spam and non-phishing as non-phishing (0)\nReturn ONLY the integer 1 or 0. Do not provide any explanation or additional text.\n### Example 1:\nEmail Content:\n\"hey billy while we were out you said that you felt insecure about your manhood i noticed in the toilets you were quite small in that area but not to worry that website that i was telling you about is my secret weapon to an extra escapenumber inches trust me girls love bigger ones i've had escapenumber times as many chicks since i used these pills a year ago the package i used was the escapenumber month supply one and its worth every cent and more the website is http ctmay com ring me on the weekend and we will go out and drink again and let you know some more secrets later dude brad\"\nYour Response: 0\n### Example 2:\nEmail Content:\n\"Your account has been hacked click the link below URLFOUND\"\nYour Response: 1\n### Now classify this email:\nEmail Content:\n\"{content}\"\nYour Response: [/INST]\n\"\"\"\n\ndef truncate_content(content, max_length=600):\n    return content[:max_length] + \"...\" if len(content) > max_length else content\n\ndef prepare_prompt(content):\n    return prompt_template.format(content=truncate_content(content))\n\ndef standardize_output(result):\n    result = result.strip()\n    if '1' in result:\n        return 1\n    elif '0' in result:\n        return 0\n    else:\n        return -1  # Invalid output\n\n# Parameters\nbatch_size = 16\nmax_seq_length = 1024\n\n# Initialize lists to store predictions\npredictions = []\nunstandardized_predictions = []\n\n# Process the test data in batches\nprint(\"Starting inference on the test dataset...\")\nfor i in tqdm(range(0, len(X_test), batch_size), desc=\"Predicting\", unit=\"batch\"):\n    batch_contents = X_test[i:i + batch_size]\n    batch_prompts = [prepare_prompt(content) for content in batch_contents]\n    \n    # Tokenize the prompts\n    inputs = tokenizer(\n        batch_prompts,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=max_seq_length,\n        padding=True\n    ).to(\"cuda\")\n    \n    # Generate outputs\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=2,  # Limit the output length\n            do_sample=False,\n            num_beams=1,\n            use_cache=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode outputs and standardize predictions\n    for idx in range(len(batch_contents)):\n        output = outputs[idx]\n        input_length = inputs['input_ids'].shape[1]\n        generated_tokens = output[input_length:]\n        sample_prediction = tokenizer.decode(\n            generated_tokens,\n            skip_special_tokens=True\n        )\n        unstandardized_predictions.append(sample_prediction)\n        predictions.append(standardize_output(sample_prediction))\n\n# Handle invalid predictions\nvalid_indices = [i for i, pred in enumerate(predictions) if pred != -1]\nvalid_predictions = [predictions[i] for i in valid_indices]\nvalid_y_true = [y_test[i] for i in valid_indices]\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(valid_y_true, valid_predictions, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:47:48.146569Z","iopub.execute_input":"2024-09-27T06:47:48.146990Z","iopub.status.idle":"2024-09-27T07:52:17.168825Z","shell.execute_reply.started":"2024-09-27T06:47:48.146940Z","shell.execute_reply":"2024-09-27T07:52:17.167870Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Starting inference on the test dataset...\n","output_type":"stream"},{"name":"stderr","text":"Predicting:   0%|          | 0/63 [00:00<?, ?batch/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\nPredicting:   2%|▏         | 1/63 [01:41<1:44:56, 101.56s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:   3%|▎         | 2/63 [02:33<1:13:47, 72.58s/batch] A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:   5%|▍         | 3/63 [04:03<1:20:11, 80.19s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:   6%|▋         | 4/63 [05:34<1:23:05, 84.50s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:   8%|▊         | 5/63 [06:26<1:10:23, 72.81s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  10%|▉         | 6/63 [08:03<1:17:03, 81.11s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  11%|█         | 7/63 [08:51<1:05:39, 70.35s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  13%|█▎        | 8/63 [09:42<58:42, 64.05s/batch]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  14%|█▍        | 9/63 [10:32<53:41, 59.66s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  16%|█▌        | 10/63 [11:22<50:13, 56.87s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  17%|█▋        | 11/63 [12:46<56:15, 64.91s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  19%|█▉        | 12/63 [14:21<1:03:07, 74.26s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  21%|██        | 13/63 [15:53<1:06:17, 79.55s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  22%|██▏       | 14/63 [16:44<57:59, 71.02s/batch]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  24%|██▍       | 15/63 [17:33<51:22, 64.23s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  25%|██▌       | 16/63 [18:26<47:45, 60.96s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  27%|██▋       | 17/63 [19:18<44:44, 58.37s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  29%|██▊       | 18/63 [20:09<42:07, 56.16s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  30%|███       | 19/63 [21:02<40:17, 54.95s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  32%|███▏      | 20/63 [21:50<37:59, 53.02s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  33%|███▎      | 21/63 [22:42<36:56, 52.78s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  35%|███▍      | 22/63 [23:35<36:03, 52.77s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  37%|███▋      | 23/63 [24:25<34:36, 51.91s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  38%|███▊      | 24/63 [25:17<33:50, 52.07s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  40%|███▉      | 25/63 [26:53<41:12, 65.06s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  41%|████▏     | 26/63 [28:25<45:11, 73.29s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  43%|████▎     | 27/63 [29:17<40:10, 66.95s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  44%|████▍     | 28/63 [30:08<36:10, 62.01s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  46%|████▌     | 29/63 [31:38<39:50, 70.30s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  48%|████▊     | 30/63 [32:28<35:19, 64.22s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  49%|████▉     | 31/63 [33:16<31:39, 59.36s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  51%|█████     | 32/63 [34:04<28:58, 56.08s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  52%|█████▏    | 33/63 [34:53<26:57, 53.92s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  54%|█████▍    | 34/63 [35:43<25:31, 52.80s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  56%|█████▌    | 35/63 [36:35<24:35, 52.69s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  57%|█████▋    | 36/63 [37:28<23:40, 52.60s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  59%|█████▊    | 37/63 [38:20<22:46, 52.54s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  60%|██████    | 38/63 [39:13<21:54, 52.58s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  62%|██████▏   | 39/63 [40:06<21:03, 52.65s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  63%|██████▎   | 40/63 [40:58<20:05, 52.41s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  65%|██████▌   | 41/63 [41:56<19:54, 54.30s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  67%|██████▋   | 42/63 [42:44<18:20, 52.43s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  68%|██████▊   | 43/63 [43:39<17:42, 53.10s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  70%|██████▉   | 44/63 [44:33<16:54, 53.41s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  71%|███████▏  | 45/63 [45:28<16:06, 53.69s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  73%|███████▎  | 46/63 [46:21<15:09, 53.49s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  75%|███████▍  | 47/63 [47:13<14:10, 53.14s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  76%|███████▌  | 48/63 [48:01<12:54, 51.65s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  78%|███████▊  | 49/63 [49:17<13:43, 58.81s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  79%|███████▉  | 50/63 [50:05<12:05, 55.79s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  81%|████████  | 51/63 [50:54<10:43, 53.59s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  83%|████████▎ | 52/63 [51:43<09:36, 52.40s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  84%|████████▍ | 53/63 [52:33<08:35, 51.54s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  86%|████████▌ | 54/63 [54:01<09:21, 62.37s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  87%|████████▋ | 55/63 [55:38<09:42, 72.84s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  89%|████████▉ | 56/63 [56:27<07:40, 65.79s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  90%|█████████ | 57/63 [57:59<07:21, 73.64s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  92%|█████████▏| 58/63 [58:49<05:31, 66.39s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  94%|█████████▎| 59/63 [1:00:19<04:53, 73.50s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  95%|█████████▌| 60/63 [1:01:27<03:35, 71.94s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  97%|█████████▋| 61/63 [1:02:17<02:10, 65.36s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting:  98%|█████████▊| 62/63 [1:03:56<01:15, 75.38s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nPredicting: 100%|██████████| 63/63 [1:04:21<00:00, 61.30s/batch]","output_type":"stream"},{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9330    0.9653      1000\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.9330      1000\n   macro avg     0.5000    0.4665    0.4827      1000\nweighted avg     1.0000    0.9330    0.9653      1000\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Push to Huggingface","metadata":{}},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n\n\n# # Log in to your Hugging Face account\n# notebook_login()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T07:53:42.186372Z","iopub.execute_input":"2024-09-27T07:53:42.187079Z","iopub.status.idle":"2024-09-27T07:53:42.344317Z","shell.execute_reply.started":"2024-09-27T07:53:42.187036Z","shell.execute_reply":"2024-09-27T07:53:42.343289Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76b6ef8abc194913865ed5917b27bb7a"}},"metadata":{}}]},{"cell_type":"code","source":"# # Load the fine-tuned model\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"./mistral_finetuned_merged\",\n#     device_map=\"auto\",\n#     torch_dtype=torch.float16,\n#     trust_remote_code=True,\n# )\n\n# # Load the tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"./mistral_finetuned_merged\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T07:54:50.147082Z","iopub.execute_input":"2024-09-27T07:54:50.147468Z","iopub.status.idle":"2024-09-27T07:54:53.447951Z","shell.execute_reply.started":"2024-09-27T07:54:50.147433Z","shell.execute_reply":"2024-09-27T07:54:53.447003Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"}]},{"cell_type":"code","source":"# repo_name = \"Rsood/mistral-instruct-v2-phishing-detection\"  # Replace with your username and desired repo name\n# # Push the model to Hugging Face Hub\n# model.push_to_hub(repo_name)\n\n# # Push the tokenizer to Hugging Face Hub\n# tokenizer.push_to_hub(repo_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:57:35.007646Z","iopub.execute_input":"2024-09-27T08:57:35.008520Z","iopub.status.idle":"2024-09-27T08:58:11.099532Z","shell.execute_reply.started":"2024-09-27T08:57:35.008468Z","shell.execute_reply":"2024-09-27T08:58:11.098528Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Rsood/mistral-instruct-v2-phishing-detection/commit/d36c1b52237c7824e1595401b60090f1e17daabd', commit_message='Upload tokenizer', commit_description='', oid='d36c1b52237c7824e1595401b60090f1e17daabd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rsood/mistral-instruct-v2-phishing-detection', endpoint='https://huggingface.co', repo_type='model', repo_id='Rsood/mistral-instruct-v2-phishing-detection'), pr_revision=None, pr_num=None)"},"metadata":{}}]}]}