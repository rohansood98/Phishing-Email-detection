{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9106778,"sourceType":"datasetVersion","datasetId":5496177},{"sourceId":9106973,"sourceType":"datasetVersion","datasetId":5496271}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\nimport pickle\nimport os\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport keras_tuner as kt\nimport matplotlib.pyplot as plt\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\n# Download NLTK data files (if not already installed)\nnltk.download('stopwords')\nnltk.download('punkt')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:10:15.290194Z","iopub.execute_input":"2024-08-19T21:10:15.290831Z","iopub.status.idle":"2024-08-19T21:10:36.131999Z","shell.execute_reply.started":"2024-08-19T21:10:15.290800Z","shell.execute_reply":"2024-08-19T21:10:36.131044Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-19 21:10:19.784585: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-19 21:10:19.784683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-19 21:10:19.915044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text, stop_words):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n    text = text.lower()\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return ' '.join(filtered_text)\n\ndef stem_text(text):\n    stemmer = PorterStemmer()\n    word_tokens = word_tokenize(text)\n    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n    return ' '.join(stemmed_text)\n\ndef ml_preprocess(df):\n    stop_words = set(stopwords.words('english'))\n    df['subject'].fillna('', inplace=True)\n    df['content'] = df['subject'] + ' ' + df['body']\n    df['content'] = df['content'].apply(lambda x: clean_text(x, stop_words))\n    df['content'] = df['content'].apply(stem_text)\n    df = df[['label', 'content']]\n    return df\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/phishing-email-dataset-nazario-5-and-trec07/Nazario_5.csv\")\n\n# Preprocess the dataset\ndf = ml_preprocess(df)\n\n# Split the data into training and testing sets\nX = df['content']\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:10:36.133610Z","iopub.execute_input":"2024-08-19T21:10:36.134470Z","iopub.status.idle":"2024-08-19T21:11:30.708986Z","shell.execute_reply.started":"2024-08-19T21:10:36.134434Z","shell.execute_reply":"2024-08-19T21:11:30.708193Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3935423188.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['subject'].fillna('', inplace=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Traditional ML Models","metadata":{}},{"cell_type":"code","source":"# Define the models\nrf_model = RandomForestClassifier(random_state=42)\nsvm_model = SVC(probability=True, random_state=42)\nknn_model = KNeighborsClassifier()\n\n# Create pipelines for each model\nrf_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', rf_model)\n])\n\nsvm_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', svm_model)\n])\n\nknn_pipeline = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', knn_model)\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:47:52.701902Z","iopub.execute_input":"2024-08-05T03:47:52.702279Z","iopub.status.idle":"2024-08-05T03:47:52.708754Z","shell.execute_reply.started":"2024-08-05T03:47:52.702245Z","shell.execute_reply":"2024-08-05T03:47:52.707831Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Random Forrest","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for Grid Search\nrf_params = {\n    'classifier__n_estimators': [200, 300, 400],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10]\n}\n\n# Perform Grid Search CV for Random Forest\nrf_grid = GridSearchCV(rf_pipeline, rf_params, cv=5, n_jobs=-1, verbose=1)\nrf_grid.fit(X_train, y_train)\nprint(\"Best parameters for Random Forest:\", rf_grid.best_params_)\nrf_pred = rf_grid.predict(X_test)\nrf_pred_prob = rf_grid.predict_proba(X_test)[:, 1]\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:47:52.711405Z","iopub.execute_input":"2024-08-05T03:47:52.711758Z","iopub.status.idle":"2024-08-05T03:52:15.723360Z","shell.execute_reply.started":"2024-08-05T03:47:52.711728Z","shell.execute_reply":"2024-08-05T03:52:15.722232Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 36 candidates, totalling 180 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Best parameters for Random Forest: {'classifier__max_depth': None, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 300}\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    0.9840    0.9904       313\n           1     0.9836    0.9967    0.9901       300\n\n    accuracy                         0.9902       613\n   macro avg     0.9902    0.9903    0.9902       613\nweighted avg     0.9903    0.9902    0.9902       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for SVM\nsvm_params = {\n    'classifier__C': [0.1, 1, 10],\n    'classifier__kernel': ['linear', 'rbf']\n}\n# Perform Grid Search CV for SVM\nsvm_grid = GridSearchCV(svm_pipeline, svm_params, cv=5, n_jobs=-1, verbose=1)\nsvm_grid.fit(X_train, y_train)\nprint(\"Best parameters for SVM:\", svm_grid.best_params_)\nsvm_pred = svm_grid.predict(X_test)\nsvm_pred_prob = svm_grid.predict_proba(X_test)[:, 1]\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_pred, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:52:15.725061Z","iopub.execute_input":"2024-08-05T03:52:15.725462Z","iopub.status.idle":"2024-08-05T03:54:18.623949Z","shell.execute_reply.started":"2024-08-05T03:52:15.725423Z","shell.execute_reply":"2024-08-05T03:54:18.622869Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 6 candidates, totalling 30 fits\nBest parameters for SVM: {'classifier__C': 1, 'classifier__kernel': 'linear'}\nSVM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9968    0.9984       313\n           1     0.9967    1.0000    0.9983       300\n\n    accuracy                         0.9984       613\n   macro avg     0.9983    0.9984    0.9984       613\nweighted avg     0.9984    0.9984    0.9984       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters for KNN\nknn_params = {\n    'classifier__n_neighbors': [3, 5, 7],\n    'classifier__weights': ['uniform', 'distance']\n}\n# Perform Grid Search CV for KNN\nknn_grid = GridSearchCV(knn_pipeline, knn_params, cv=5, n_jobs=-1, verbose=1)\nknn_grid.fit(X_train, y_train)\nprint(\"Best parameters for KNN:\", knn_grid.best_params_)\nknn_pred = knn_grid.predict(X_test)\nknn_pred_prob = knn_grid.predict_proba(X_test)[:, 1]\nprint(\"KNN Classification Report:\")\nprint(classification_report(y_test, knn_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:54:18.625180Z","iopub.execute_input":"2024-08-05T03:54:18.625469Z","iopub.status.idle":"2024-08-05T03:56:04.501802Z","shell.execute_reply.started":"2024-08-05T03:54:18.625445Z","shell.execute_reply":"2024-08-05T03:56:04.500945Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 6 candidates, totalling 30 fits\nBest parameters for KNN: {'classifier__n_neighbors': 5, 'classifier__weights': 'distance'}\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9553    0.9771       313\n           1     0.9554    1.0000    0.9772       300\n\n    accuracy                         0.9772       613\n   macro avg     0.9777    0.9776    0.9772       613\nweighted avg     0.9782    0.9772    0.9772       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save Traditional ML Models","metadata":{}},{"cell_type":"code","source":"# Save traditional ML models\nif not os.path.exists('models'):\n    os.makedirs('models')\n\nwith open('models/rf_pipeline.pkl', 'wb') as f:\n    pickle.dump(rf_grid.best_estimator_, f)\n\nwith open('models/svm_pipeline.pkl', 'wb') as f:\n    pickle.dump(svm_grid.best_estimator_, f)\n\nwith open('models/knn_pipeline.pkl', 'wb') as f:\n    pickle.dump(knn_grid.best_estimator_, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:56:04.503266Z","iopub.execute_input":"2024-08-05T03:56:04.503992Z","iopub.status.idle":"2024-08-05T03:56:04.660366Z","shell.execute_reply.started":"2024-08-05T03:56:04.503951Z","shell.execute_reply":"2024-08-05T03:56:04.659597Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Models","metadata":{}},{"cell_type":"code","source":"\n# Tokenize and pad sequences for deep learning models\nmax_words = 10000\nmax_len = 500\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n\ndef build_cnn_model(hp):\n    model = Sequential()\n    model.add(Embedding(max_words, 128, input_length=max_len))\n    model.add(Conv1D(\n        filters=hp.Int('filters', min_value=32, max_value=256, step=32),\n        kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]),\n        activation='relu'\n    ))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(\n        units=hp.Int('units', min_value=32, max_value=256, step=32),\n        activation='relu'\n    ))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\ndef build_lstm_model(hp):\n    model = Sequential()\n    model.add(Embedding(max_words, 128, input_length=max_len))\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(\n        units=hp.Int('lstm_units', min_value=32, max_value=256, step=32),\n        dropout=0.2,\n        recurrent_dropout=0.2\n    ))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:51:35.426005Z","iopub.execute_input":"2024-08-05T04:51:35.426947Z","iopub.status.idle":"2024-08-05T04:51:37.476938Z","shell.execute_reply.started":"2024-08-05T04:51:35.426913Z","shell.execute_reply":"2024-08-05T04:51:37.475874Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for CNN\ncnn_tuner = kt.Hyperband(\n    build_cnn_model,\n    objective='val_accuracy',\n    max_epochs=10,\n    hyperband_iterations=2,\n    directory='my_dir',\n    project_name='cnn_tuning'\n)\n\n\n\n# Search for best hyperparameters\ncnn_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n\n# Retrieve best hyperparameters for CNN\nbest_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search for CNN is complete. The optimal number of filters in the Conv1D layer is {best_cnn_hps.get('filters')},\nthe optimal kernel size is {best_cnn_hps.get('kernel_size')}, and the optimal number of units in the Dense layer is {best_cnn_hps.get('units')}.\n\"\"\")\n\n\n# Train with best hyperparameters for CNN\ncnn_model = cnn_tuner.hypermodel.build(best_cnn_hps)\ncnn_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\ncnn_model.save('models/best_cnn_model.h5')\n\ncnn_pred_prob = cnn_model.predict(X_test_pad)\ncnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\nprint(\"CNN Classification Report:\")\nprint(classification_report(y_test, cnn_pred, digits=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T03:56:06.824584Z","iopub.execute_input":"2024-08-05T03:56:06.825223Z","iopub.status.idle":"2024-08-05T04:02:37.857571Z","shell.execute_reply.started":"2024-08-05T03:56:06.825195Z","shell.execute_reply":"2024-08-05T04:02:37.856548Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Trial 60 Complete [00h 00m 08s]\nval_accuracy: 0.9918533563613892\n\nBest val_accuracy So Far: 0.9938900470733643\nTotal elapsed time: 00h 06m 22s\n\nThe hyperparameter search for CNN is complete. The optimal number of filters in the Conv1D layer is 256,\nthe optimal kernel size is 3, and the optimal number of units in the Dense layer is 96.\n\nEpoch 1/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.7364 - loss: 0.5566 - val_accuracy: 0.9796 - val_loss: 0.0684\nEpoch 2/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9858 - loss: 0.0465 - val_accuracy: 0.9817 - val_loss: 0.0418\nEpoch 3/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9994 - loss: 0.0047 - val_accuracy: 0.9857 - val_loss: 0.0401\nEpoch 4/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9857 - val_loss: 0.0355\nEpoch 5/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.9249e-04 - val_accuracy: 0.9857 - val_loss: 0.0348\nEpoch 6/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.0786e-04 - val_accuracy: 0.9857 - val_loss: 0.0348\nEpoch 7/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4738e-04 - val_accuracy: 0.9878 - val_loss: 0.0357\nEpoch 8/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0087e-04 - val_accuracy: 0.9878 - val_loss: 0.0360\nEpoch 9/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2432e-04 - val_accuracy: 0.9878 - val_loss: 0.0361\nEpoch 10/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0436e-04 - val_accuracy: 0.9878 - val_loss: 0.0359\n\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\nCNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9936    0.9904    0.9920       313\n           1     0.9900    0.9933    0.9917       300\n\n    accuracy                         0.9918       613\n   macro avg     0.9918    0.9919    0.9918       613\nweighted avg     0.9918    0.9918    0.9918       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning for LSTM\nlstm_tuner = kt.Hyperband(\n    build_lstm_model,\n    objective='val_accuracy',\n    max_epochs=10,\n    hyperband_iterations=2,\n    directory='my_dir',\n    project_name='lstm_tuning'\n)\n\n# Search for best hyperparameters\nlstm_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n\n# Retrieve best hyperparameters for LSTM\nbest_lstm_hps = lstm_tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is {best_lstm_hps.get('lstm_units')}.\n\"\"\")\n\n# Train with best hyperparameters for LSTM\nlstm_model = lstm_tuner.hypermodel.build(best_lstm_hps)\nlstm_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\nlstm_model.save('models/best_lstm_model.h5')\n\nlstm_pred_prob = lstm_model.predict(X_test_pad)\nlstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test, lstm_pred, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:02:37.860981Z","iopub.execute_input":"2024-08-05T04:02:37.861441Z","iopub.status.idle":"2024-08-05T04:25:29.259631Z","shell.execute_reply.started":"2024-08-05T04:02:37.861406Z","shell.execute_reply":"2024-08-05T04:25:29.258550Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Trial 8 Complete [00h 01m 37s]\nval_accuracy: 0.9877800345420837\n\nBest val_accuracy So Far: 0.9898167252540588\nTotal elapsed time: 00h 14m 08s\n\nThe hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is 128.\n\nEpoch 1/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 716ms/step - accuracy: 0.7784 - loss: 0.5090 - val_accuracy: 0.9776 - val_loss: 0.0797\nEpoch 2/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 711ms/step - accuracy: 0.9819 - loss: 0.0600 - val_accuracy: 0.9613 - val_loss: 0.1293\nEpoch 3/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 705ms/step - accuracy: 0.9892 - loss: 0.0500 - val_accuracy: 0.9837 - val_loss: 0.0758\nEpoch 4/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 716ms/step - accuracy: 0.9978 - loss: 0.0107 - val_accuracy: 0.9878 - val_loss: 0.0460\nEpoch 5/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 712ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9817 - val_loss: 0.1126\nEpoch 6/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 716ms/step - accuracy: 0.9983 - loss: 0.0127 - val_accuracy: 0.9898 - val_loss: 0.0492\nEpoch 7/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 711ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.9837 - val_loss: 0.0658\nEpoch 8/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 720ms/step - accuracy: 0.9998 - loss: 0.0027 - val_accuracy: 0.9898 - val_loss: 0.0587\nEpoch 9/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 711ms/step - accuracy: 0.9998 - loss: 0.0024 - val_accuracy: 0.9878 - val_loss: 0.0757\nEpoch 10/10\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 706ms/step - accuracy: 0.9994 - loss: 0.0015 - val_accuracy: 0.9837 - val_loss: 0.0672\n\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9812    1.0000    0.9905       313\n           1     1.0000    0.9800    0.9899       300\n\n    accuracy                         0.9902       613\n   macro avg     0.9906    0.9900    0.9902       613\nweighted avg     0.9904    0.9902    0.9902       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LMMs","metadata":{}},{"cell_type":"code","source":"# Define Hugging Face model training function with progress bar\ndef train_hf_model(model_name, tokenizer_class, model_class, X_train, y_train, X_test, y_test, save_path):\n    os.environ['WANDB_DISABLED'] = 'true'\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=2)\n\n    train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512)\n    test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512)\n\n    train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': y_train.tolist()})\n    test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': y_test.tolist()})\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        tokenizer=tokenizer,\n    )\n\n    print(f\"Training {model_name} model...\")\n    trainer.train()\n\n    # Evaluate the model\n    predictions = trainer.predict(test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n    print(f\"{model_name} Classification Report:\")\n    print(classification_report(y_test, preds, digits=4))\n\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:58:18.966030Z","iopub.execute_input":"2024-08-05T04:58:18.966400Z","iopub.status.idle":"2024-08-05T04:58:18.976578Z","shell.execute_reply.started":"2024-08-05T04:58:18.966370Z","shell.execute_reply":"2024-08-05T04:58:18.975597Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"# Train and save BERT model\ntrain_hf_model(\n    model_name='bert-base-uncased',\n    tokenizer_class=BertTokenizer,\n    model_class=BertForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/bert_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T04:58:24.260877Z","iopub.execute_input":"2024-08-05T04:58:24.261499Z","iopub.status.idle":"2024-08-05T05:08:11.714792Z","shell.execute_reply.started":"2024-08-05T04:58:24.261468Z","shell.execute_reply":"2024-08-05T05:08:11.713964Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training bert-base-uncased model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 06:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.674400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.677800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.654500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.648500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.588300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.490800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.433900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.384600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.278700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.257700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.216100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.181600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.101200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.239600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.043200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.241400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.155300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.075900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.123000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.091300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.460200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.074700</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.074000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.143000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.232500</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.112100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.127300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.110700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.248900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.187700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.204200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.067600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.049100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.049700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.028500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.069300</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.097300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.263500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.032100</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.121600</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.109500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.087700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.276800</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.104700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.229500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.181500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.039200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.126400</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.080100</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.070800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.125900</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.094100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.147700</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"bert-base-uncased Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9936    0.9936    0.9936       313\n           1     0.9933    0.9933    0.9933       300\n\n    accuracy                         0.9935       613\n   macro avg     0.9935    0.9935    0.9935       613\nweighted avg     0.9935    0.9935    0.9935       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"# Train and save DistilBERT model\ntrain_hf_model(\n    model_name='distilbert-base-uncased',\n    tokenizer_class=DistilBertTokenizer,\n    model_class=DistilBertForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/distilbert_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T05:10:59.002625Z","iopub.execute_input":"2024-08-05T05:10:59.003013Z","iopub.status.idle":"2024-08-05T05:17:30.886662Z","shell.execute_reply.started":"2024-08-05T05:10:59.002980Z","shell.execute_reply":"2024-08-05T05:17:30.885812Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187a9df3ac1a4ea3808969bea087f160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824482372b3d4c4fa0a923ab7652db66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d191c1e09cb4058ac4edc5d2b8ec11e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565dd986736744698a882b0dff79a276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6752fc3e38ba482b9e3ba7d7c002b94a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training distilbert-base-uncased model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 03:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.686900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.696600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.687300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.678400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.677000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.635400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.583400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.513300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.398000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.325800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.233300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.227500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.104400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.227000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.052600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.127600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.053700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.123500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.071300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.393200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.162000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.035500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.082800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.373100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.130800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.075700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.007900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.060800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.079700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.085100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.087900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.086900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.278500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.176300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.162100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.064900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.135500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.003000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.064400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.054400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.185200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.170800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.057800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.075800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.000400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"distilbert-base-uncased Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    0.9904    0.9936       313\n           1     0.9901    0.9967    0.9934       300\n\n    accuracy                         0.9935       613\n   macro avg     0.9934    0.9935    0.9935       613\nweighted avg     0.9935    0.9935    0.9935       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"# Train and save RoBERTa model\ntrain_hf_model(\n    model_name='roberta-base',\n    tokenizer_class=RobertaTokenizer,\n    model_class=RobertaForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/roberta_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T05:19:18.583822Z","iopub.execute_input":"2024-08-05T05:19:18.584556Z","iopub.status.idle":"2024-08-05T05:27:18.810235Z","shell.execute_reply.started":"2024-08-05T05:19:18.584510Z","shell.execute_reply":"2024-08-05T05:27:18.809369Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929afd3b140f4d51beb64f2fa6bb2ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980df87d597e4e1cbe1d6822511577ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7c88b68f8e47eb8f34cb9c63449123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4efc9f2e604cfa98b17980607d0fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617b346a61dc4292b95c95ada8c77cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc8e9e5d29749b0b34a1d9fffde33bf"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training roberta-base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 06:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.709600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.694100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.680200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.682900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.656300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.625600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.467100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.304800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.226400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.227900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.322600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.123100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.233000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.327400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.036900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.462500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.345800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.466900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.065600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.102600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.406600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.199600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.142600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.127600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.137500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.200100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.181300</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.122900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.032900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.064900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.284200</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.192500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.215400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.060600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.036600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.321200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.182400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.162400</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.096600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.150500</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.261100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.137200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.231800</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.114100</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.189500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.138900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.180400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.178300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.026900</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.317700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.084500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.058400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.054900</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.149000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.041400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.096000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.106300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.054700</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.069400</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.000700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"roberta-base Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9968    0.9872    0.9920       313\n           1     0.9868    0.9967    0.9917       300\n\n    accuracy                         0.9918       613\n   macro avg     0.9918    0.9919    0.9918       613\nweighted avg     0.9919    0.9918    0.9918       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT Large","metadata":{}},{"cell_type":"code","source":"# Train and save BERT Large model\ntrain_hf_model(\n    model_name='bert-large-uncased',\n    tokenizer_class=BertTokenizer,\n    model_class=BertForSequenceClassification,\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    save_path='models/bert_large_model'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T05:27:53.876060Z","iopub.execute_input":"2024-08-05T05:27:53.876408Z","iopub.status.idle":"2024-08-05T05:54:02.360676Z","shell.execute_reply.started":"2024-08-05T05:27:53.876381Z","shell.execute_reply":"2024-08-05T05:54:02.359858Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5efe46ed04e44d229a65859be02e4fa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e790b47c13ec43a08053768052a9de12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0772b9dad363456683d42432668a161d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f83efe8efa4427b931a74d4b1ab7476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9be9e0fb733498e888ae7c5abeac178"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Training bert-large-uncased model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='921' max='921' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [921/921 22:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.711300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.677400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.635100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.653700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.584900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.551200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.460100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.403800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.396700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.278600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.251300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.138700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.223100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.166200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.092400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.144700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.146300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.088300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.103000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.138400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.104900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.591800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.068500</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.313100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.106600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.160100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.058400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.312300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.154700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.065400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.114400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.106700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.171100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.180400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.125300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.829600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.428400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.422500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.319300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.808200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.792000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.725600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.729700</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.522200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.168600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.047800</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.086300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.192300</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.145300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.054000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.133100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.061600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.092400</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.079100</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.052600</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.105700</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.190900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.092000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.001400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"bert-large-uncased Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9873    0.9904    0.9888       313\n           1     0.9900    0.9867    0.9883       300\n\n    accuracy                         0.9886       613\n   macro avg     0.9886    0.9885    0.9886       613\nweighted avg     0.9886    0.9886    0.9886       613\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Plot ROC curves\ndef plot_roc_curve(y_true, y_pred_prob, model_name):\n    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'Receiver Operating Characteristic - {model_name}')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    print(f'{model_name} AUROC: {roc_auc:.2f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Download NLTK data files (if not already installed)\nnltk.download('stopwords')\nnltk.download('punkt')\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef clean_text(text, stop_words):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n    text = text.lower()\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return ' '.join(filtered_text)\n\ndef stem_text(text):\n    stemmer = PorterStemmer()\n    word_tokens = word_tokenize(text)\n    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n    return ' '.join(stemmed_text)\n\ndef ml_preprocess(df):\n    stop_words = set(stopwords.words('english'))\n    df['text'] = df['text'].apply(lambda x: clean_text(x, stop_words))\n    df['text'] = df['text'].apply(stem_text)\n    return df\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/phishing-email-dataset-nazario-5-and-trec07/email_text.csv\")  # Update with the actual path\n\n# Preprocess the dataset\ndf = ml_preprocess(df)\n\n# Split the dataset into samples where the label is 1 (spam emails)\ndf_label_1 = df[df['label'] == 1]\n\n# Change the label of these samples to 0\ndf_label_1['label'] = 0\n\n# Define the data for testing\nX_test = df_label_1['text']\ny_test = df_label_1['label']\n\n# Tokenize and pad sequences for deep learning models\nmax_words = 10000\nmax_len = 500\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_test)\n\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:11:30.710022Z","iopub.execute_input":"2024-08-19T21:11:30.710324Z","iopub.status.idle":"2024-08-19T21:20:59.700590Z","shell.execute_reply.started":"2024-08-19T21:11:30.710276Z","shell.execute_reply":"2024-08-19T21:20:59.699775Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/279759549.py:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_label_1['label'] = 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load traditional ML models\nwith open('/kaggle/working/models/rf_pipeline.pkl', 'rb') as f:\n    rf_model = pickle.load(f)\n\nwith open('/kaggle/working/models/svm_pipeline.pkl', 'rb') as f:\n    svm_model = pickle.load(f)\n\nwith open('/kaggle/working/models/knn_pipeline.pkl', 'rb') as f:\n    knn_model = pickle.load(f)\n    \n# Predict and print classification report and ROC curve for Random Forest\nrf_pred = rf_model.predict(X_test)\nrf_pred_prob = rf_model.predict_proba(X_test)[:, 1]\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred, digits=4))\n\n# Predict and print classification report and ROC curve for SVM\nsvm_pred = svm_model.predict(X_test)\nsvm_pred_prob = svm_model.predict_proba(X_test)[:, 1]\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_test, svm_pred, digits=4))\n\n# Predict and print classification report and ROC curve for KNN\nknn_pred = knn_model.predict(X_test)\nknn_pred_prob = knn_model.predict_proba(X_test)[:, 1]\nprint(\"KNN Classification Report:\")\nprint(classification_report(y_test, knn_pred, digits=4))\n\n\n\n\n\n\n\n# # Plot ROC curves for all models\n# plot_roc_curve(y_test, rf_pred_prob, 'Random Forest')\n# plot_roc_curve(y_test, svm_pred_prob, 'SVM')\n# plot_roc_curve(y_test, knn_pred_prob, 'KNN')\n# plot_roc_curve(y_test, cnn_pred_prob, 'CNN')\n# plot_roc_curve(y_test, lstm_pred_prob, 'LSTM')\n# plot_roc_curve(y_test, bert_pred_prob, 'BERT')\n# plot_roc_curve(y_test, distilbert_pred_prob, 'DistilBERT')\n# plot_roc_curve(y_test, roberta_pred_prob, 'RoBERTa')","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:49:32.337180Z","iopub.execute_input":"2024-08-19T20:49:32.337612Z","iopub.status.idle":"2024-08-19T20:56:44.377954Z","shell.execute_reply.started":"2024-08-19T20:49:32.337583Z","shell.execute_reply":"2024-08-19T20:56:44.377057Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Random Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7081    0.8291     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7081     29923\n   macro avg     0.5000    0.3540    0.4145     29923\nweighted avg     1.0000    0.7081    0.8291     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"SVM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9059    0.9506     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.9059     29923\n   macro avg     0.5000    0.4530    0.4753     29923\nweighted avg     1.0000    0.9059    0.9506     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"KNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.6292    0.7724     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.6292     29923\n   macro avg     0.5000    0.3146    0.3862     29923\nweighted avg     1.0000    0.6292    0.7724     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load deep learning models\ncnn_model = load_model('/kaggle/working/models/best_cnn_model.h5')\nlstm_model = load_model('/kaggle/working/models/best_lstm_model.h5')\n\n# Predict and print classification report and ROC curve for CNN\ncnn_pred_prob = cnn_model.predict(X_test_pad)\ncnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\nprint(\"CNN Classification Report:\")\nprint(classification_report(y_test, cnn_pred, digits=4))\n\n# Predict and print classification report and ROC curve for LSTM\nlstm_pred_prob = lstm_model.predict(X_test_pad)\nlstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test, lstm_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:59:34.867048Z","iopub.execute_input":"2024-08-19T20:59:34.867356Z","iopub.status.idle":"2024-08-19T21:02:20.615612Z","shell.execute_reply.started":"2024-08-19T20:59:34.867331Z","shell.execute_reply":"2024-08-19T21:02:20.614690Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[1m936/936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nCNN Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.6972    0.8216     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.6972     29923\n   macro avg     0.5000    0.3486    0.4108     29923\nweighted avg     1.0000    0.6972    0.8216     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m936/936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 174ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.8786    0.9354     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8786     29923\n   macro avg     0.5000    0.4393    0.4677     29923\nweighted avg     1.0000    0.8786    0.9354     29923\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_hf_model(model, tokenizer, X_test):\n    encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n    outputs = model(**encodings)\n    predictions = torch.argmax(outputs.logits, dim=1).detach().numpy()\n    pred_prob = torch.softmax(outputs.logits, dim=1).detach().numpy()[:, 1]\n    return predictions, pred_prob\n\ndef predict_hf_model_batch(model, tokenizer, dataloader, device):\n    model = model.to(device)\n    model.eval()\n    all_preds = []\n    all_probs = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Predicting\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n            all_preds.extend(preds)\n            all_probs.extend(probs)\n    return np.array(all_preds), np.array(all_probs)\n# Parameters\nBATCH_SIZE = 64\nMAX_LENGTH = 512\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:20:59.702552Z","iopub.execute_input":"2024-08-19T21:20:59.702846Z","iopub.status.idle":"2024-08-19T21:20:59.748866Z","shell.execute_reply.started":"2024-08-19T21:20:59.702821Z","shell.execute_reply":"2024-08-19T21:20:59.747990Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Load Hugging Face models using safetensors\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/working/models/bert_model')\nbert_model = BertForSequenceClassification.from_pretrained('/kaggle/working/models/bert_model', use_safetensors=True)\n\n# Create DataLoader\ntest_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), bert_tokenizer, MAX_LENGTH)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n\n# Predict and print classification report and ROC curve for BERT\nbert_pred, bert_pred_prob = predict_hf_model_batch(bert_model, bert_tokenizer, test_dataloader, device)\nprint(\"BERT Classification Report:\")\nprint(classification_report(y_test, bert_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:20:59.750121Z","iopub.execute_input":"2024-08-19T21:20:59.750683Z","iopub.status.idle":"2024-08-19T21:32:30.531740Z","shell.execute_reply.started":"2024-08-19T21:20:59.750647Z","shell.execute_reply":"2024-08-19T21:32:30.530682Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 468/468 [11:29<00:00,  1.47s/batch]","output_type":"stream"},{"name":"stdout","text":"BERT Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.8589    0.9241     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8589     29923\n   macro avg     0.5000    0.4294    0.4620     29923\nweighted avg     1.0000    0.8589    0.9241     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('/kaggle/working/models/distilbert_model')\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('/kaggle/working/models/distilbert_model', use_safetensors=True)\n\n# Predict and print classification report and ROC curve for DistilBERT\ndistilbert_test_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), distilbert_tokenizer, MAX_LENGTH)\ndistilbert_test_dataloader = DataLoader(distilbert_test_dataset, batch_size=BATCH_SIZE)\n\ndistilbert_pred, distilbert_pred_prob = predict_hf_model_batch(distilbert_model, distilbert_tokenizer, distilbert_test_dataloader, device)\nprint(\"DistilBERT Classification Report:\")\nprint(classification_report(y_test, distilbert_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:32:30.533150Z","iopub.execute_input":"2024-08-19T21:32:30.534051Z","iopub.status.idle":"2024-08-19T21:40:10.728434Z","shell.execute_reply.started":"2024-08-19T21:32:30.534021Z","shell.execute_reply":"2024-08-19T21:40:10.727531Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 468/468 [07:39<00:00,  1.02batch/s]","output_type":"stream"},{"name":"stdout","text":"DistilBERT Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7095    0.8301     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7095     29923\n   macro avg     0.5000    0.3547    0.4150     29923\nweighted avg     1.0000    0.7095    0.8301     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"roberta_tokenizer = RobertaTokenizer.from_pretrained('/kaggle/working/models/roberta_model')\nroberta_model = RobertaForSequenceClassification.from_pretrained('models/roberta_model', use_safetensors=True)\n\n# Predict and print classification report and ROC curve for RoBERTa\nroberta_test_dataset = CustomDataset(X_test.tolist(), y_test.tolist(), roberta_tokenizer, MAX_LENGTH)\nroberta_test_dataloader = DataLoader(roberta_test_dataset, batch_size=BATCH_SIZE)\n\nroberta_pred, roberta_pred_prob = predict_hf_model_batch(roberta_model, roberta_tokenizer, roberta_test_dataloader, device)\nprint(\"RoBERTa Classification Report:\")\nprint(classification_report(y_test, roberta_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:40:10.729830Z","iopub.execute_input":"2024-08-19T21:40:10.730115Z","iopub.status.idle":"2024-08-19T21:50:04.001995Z","shell.execute_reply.started":"2024-08-19T21:40:10.730089Z","shell.execute_reply":"2024-08-19T21:50:04.001032Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 468/468 [09:52<00:00,  1.27s/batch]","output_type":"stream"},{"name":"stdout","text":"RoBERTa Classification Report:\n              precision    recall  f1-score   support\n\n           0     1.0000    0.7365    0.8482     29923\n           1     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.7365     29923\n   macro avg     0.5000    0.3682    0.4241     29923\nweighted avg     1.0000    0.7365    0.8482     29923\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}