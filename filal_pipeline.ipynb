{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK data files (if not already installed)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text, stop_words):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URLfound', text, flags=re.MULTILINE)\n",
    "    text = text.lower()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "def ml_preprocess(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['subject'].fillna('', inplace=True)\n",
    "    df['content'] = df['subject'] + ' ' + df['body']\n",
    "    df['content'] = df['content'].apply(lambda x: clean_text(x, stop_words))\n",
    "    df['content'] = df['content'].apply(stem_text)\n",
    "    df = df[['label', 'content']]\n",
    "    return df\n",
    "\n",
    "# Plot ROC curves\n",
    "def plot_roc_curve(y_true, y_pred_prob, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    print(f'{model_name} AUROC: {roc_auc:.2f}')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Dataset/Nazario_5.csv\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = ml_preprocess(df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df['content']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Define the models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Create pipelines for each model\n",
    "rf_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', rf_model)\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', svm_model)\n",
    "])\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', knn_model)\n",
    "])\n",
    "\n",
    "# Define the hyperparameters for Grid Search\n",
    "rf_params = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Grid Search CV for Random Forest\n",
    "rf_grid = GridSearchCV(rf_pipeline, rf_params, cv=5, n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(\"Best parameters for Random Forest:\", rf_grid.best_params_)\n",
    "rf_pred = rf_grid.predict(X_test)\n",
    "rf_pred_prob = rf_grid.predict_proba(X_test)[:, 1]\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred, digits=4))\n",
    "\n",
    "# Define the hyperparameters for SVM\n",
    "svm_params = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "# Perform Grid Search CV for SVM\n",
    "svm_grid = GridSearchCV(svm_pipeline, svm_params, cv=5, n_jobs=-1, verbose=1)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "print(\"Best parameters for SVM:\", svm_grid.best_params_)\n",
    "svm_pred = svm_grid.predict(X_test)\n",
    "svm_pred_prob = svm_grid.predict_proba(X_test)[:, 1]\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred, digits=4))\n",
    "\n",
    "# Define the hyperparameters for KNN\n",
    "knn_params = {\n",
    "    'classifier__n_neighbors': [3, 5, 7],\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "# Perform Grid Search CV for KNN\n",
    "knn_grid = GridSearchCV(knn_pipeline, knn_params, cv=5, n_jobs=-1, verbose=1)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "print(\"Best parameters for KNN:\", knn_grid.best_params_)\n",
    "knn_pred = knn_grid.predict(X_test)\n",
    "knn_pred_prob = knn_grid.predict_proba(X_test)[:, 1]\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, knn_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save models if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Save traditional ML models\n",
    "with open('models/rf_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_grid.best_estimator_, f)\n",
    "\n",
    "with open('models/svm_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_grid.best_estimator_, f)\n",
    "\n",
    "with open('models/knn_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_grid.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and pad sequences for deep learning models\n",
    "max_words = 10000\n",
    "max_len = 500\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=256, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=32, max_value=256, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=256, step=32),\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2\n",
    "    ))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning for CNN\n",
    "cnn_tuner = kt.Hyperband(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    hyperband_iterations=2,\n",
    "    directory='my_dir',\n",
    "    project_name='cnn_tuning'\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning for LSTM\n",
    "lstm_tuner = kt.Hyperband(\n",
    "    build_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    hyperband_iterations=2,\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "cnn_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n",
    "lstm_tuner.search(X_train_pad, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Retrieve best hyperparameters for CNN\n",
    "best_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search for CNN is complete. The optimal number of filters in the Conv1D layer is {best_cnn_hps.get('filters')},\n",
    "the optimal kernel size is {best_cnn_hps.get('kernel_size')}, and the optimal number of units in the Dense layer is {best_cnn_hps.get('units')}.\n",
    "\"\"\")\n",
    "\n",
    "# Retrieve best hyperparameters for LSTM\n",
    "best_lstm_hps = lstm_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is {best_lstm_hps.get('lstm_units')}.\n",
    "\"\"\")\n",
    "\n",
    "# Train with best hyperparameters for CNN\n",
    "cnn_model = cnn_tuner.hypermodel.build(best_cnn_hps)\n",
    "cnn_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\n",
    "cnn_model.save('models/best_cnn_model.h5')\n",
    "\n",
    "cnn_pred_prob = cnn_model.predict(X_test_pad)\n",
    "cnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test, cnn_pred, digits=4))\n",
    "\n",
    "# Train with best hyperparameters for LSTM\n",
    "lstm_model = lstm_tuner.hypermodel.build(best_lstm_hps)\n",
    "lstm_model.fit(X_train_pad, y_train, epochs=10, validation_split=0.2)\n",
    "lstm_model.save('models/best_lstm_model.h5')\n",
    "\n",
    "lstm_pred_prob = lstm_model.predict(X_test_pad)\n",
    "lstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\n",
    "print(\"LSTM Classification Report:\")\n",
    "print(classification_report(y_test, lstm_pred, digits=4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "plot_roc_curve(y_test, rf_pred_prob, 'Random Forest')\n",
    "plot_roc_curve(y_test, svm_pred_prob, 'SVM')\n",
    "plot_roc_curve(y_test, knn_pred_prob, 'KNN')\n",
    "plot_roc_curve(y_test, cnn_pred_prob, 'CNN')\n",
    "plot_roc_curve(y_test, lstm_pred_prob, 'LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Best parameters : The hyperparameter search for LSTM is complete. The optimal number of units in the LSTM layer is 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fw june 29 bna inc daili labor report user id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ngx failov plan hi chri tonight roll new repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>intranet site rika r new origin messag thoma p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>fw ena upstream compani inform johngerald curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>new master physic gerald staci attach workshee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content\n",
       "0      0  fw june 29 bna inc daili labor report user id ...\n",
       "1      0  ngx failov plan hi chri tonight roll new repor...\n",
       "2      0  intranet site rika r new origin messag thoma p...\n",
       "3      0  fw ena upstream compani inform johngerald curr...\n",
       "4      0  new master physic gerald staci attach workshee..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please ensure GPU runtime is selected in Colab.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available and its detailSs\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
